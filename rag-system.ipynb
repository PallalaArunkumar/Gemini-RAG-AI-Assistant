{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12172031,"sourceType":"datasetVersion","datasetId":7666079},{"sourceId":12234376,"sourceType":"datasetVersion","datasetId":7708533}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"hello world\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"python ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain_community\n!pip install pypdf\n!pip install langchain_openai\n!pip install faiss-cpu\n!pip install -U langchain-google-genai\n!pip install rank_bm25\n!pip install ragas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# data_loader.py\n\ndef load_document(filepath:str):\n  loader = PyPDFLoader(filepath)\n  documents = loader.load()\n  print(f'Loaded {len(documents)} pages from the document')\n  return documents\n\ndef split_documents(doc,chunk_size:int=1000, chunk_overlap:int=200):\n  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,length_function=len)\n  chunks = text_splitter.split_documents(doc)\n  return chunks\n\n\nif __name__ == \"__main__\":\n  loaded_document = load_document('/kaggle/input/ai-wiki/Artificial intelligence - Wikipedia.pdf')\n  text_chunks = split_documents(loaded_document)\n\n  for i,chunk in enumerate(text_chunks[:3]):\n    print(f\"Chunk {i+1} (Length: {len(chunk.page_content)} characters):\")\n    print(chunk.page_content[:500] + \"...\" if len(chunk.page_content) > 500 else chunk.page_content)\n    print(\"-\" * 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%writefile create_load_vector_store.py\n\nimport os\nimport logging\nimport time\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom typing import List, Optional\n\n\n\nlogger = logging.getLogger(__name__)\n\ndef _get_embeddings_with_retries(texts, embedding_model:GoogleGenerativeAIEmbeddings, max_retries:int=5,initial_delay:int=5):\n    \"\"\"\n    Generates embeddings for a list of texts with retry logic and exponential backoff.\n    \"\"\"\n    all_embeddings =[]\n    current_text_to_embed = list(texts)\n\n    for attempt in range(max_retries):\n        try:\n            embeddings = embedding_model.embed_documents(current_text_to_embed)\n            all_embeddings.extend(embeddings)\n\n            logger.info(f\"Successfully embedded {len(current_text_to_embed)} texts on attempt {attempt + 1}.\")\n            return all_embeddings\n        except Exception as e:\n            logger.warning(f\"Embedding attempt {attempt + 1}/{max_retries} failed: {e}\")\n            if attempt < max_retries-1:\n                wait_time = initial_delay*(2**attempt)\n                logger.info(f\"Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                logger.error(f\"Failed to embed texts after {max_retries} attempts.\")\n                raise\n\ndef create_vector_store(text_chunks,embedding_model:GoogleGenerativeAIEmbeddings,db_path:str):\n    logger.info(f\"Creating FAISS vector store at {db_path} with {len(text_chunks)} chunks...\")\n    try:\n        texts_to_embeddings = [chunk.page_content for chunk in text_chunks]\n\n        embeddings = _get_embeddings_with_retries(texts_to_embeddings,embedding_model)\n\n        #create FAISS from document and embeddings\n\n        vector_store = FAISS.from_embeddings(text_embeddings=list(zip(texts_to_embeddings, embeddings)),embedding=embedding_model)\n        vector_store.add_documents(text_chunks)\n\n        os.makedirs(db_path,exist_ok=True)\n        logger.info(f'Creating the folder{db_path}')\n        vector_store.save_local(db_path)\n        logger.info(f\"FAISS vector store created and saved successfully at {db_path}.\")\n        return vector_store\n    except Exception as e:\n        logger.error(f\"Error creating FAISS vector store: {e}\")\n        raise\n\ndef load_vector_store(embedding_model:GoogleGenerativeAIEmbeddings, db_path:str):\n    logger.info(f\"Loading FAISS vector store from {db_path}...\")\n    try:\n        vector_store = FAISS.load_local(db_path,embedding_model,allow_dangerous_deserialization=True)\n        logger.info(f\"FAISS vector store loaded successfully from {db_path}.\")\n        return vector_store\n    except Exception as e:\n         logger.error(f\"Error loading FAISS vector store: {e}\")\n         raise\n        \n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile rag_chain_gemini.py\n\nimport os\nimport logging\n\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI # For chat models like Gemini Pro\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings # For Gemini Embeddings\n# from create_load_vector_store import *\n# from data_loader import *\n\nfrom langchain_community.retrievers import BM25Retriever\nfrom langchain.retrievers import EnsembleRetriever\n\n# NEW IMPORTS FOR RE-RANKING\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker # The re-ranker itself\nfrom langchain.retrievers import ContextualCompressionRetriever # To apply the re-ranker after retrieval\n# from sentence_transformers import CrossEncoder # To load the actual cross-encoder model\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\nimport pandas as pd\nfrom datasets import Dataset\n\nfrom ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom datasets import Dataset\nimport pandas as pd\nfrom ragas.run_config import RunConfig\n\n\n\n\n # --- Configure Logging ---\nlogging.basicConfig(\n        level=logging.INFO, # Set the default logging level to INFO\n        format='%(asctime)s - %(levelname)s - %(message)s',\n        force=True\n    )\nlogger = logging.getLogger(__name__) # Get a logger instance for this module\n\ndef setup_rag_components(document:str, faiss_db_path:str,llm_model_id:str):\n\n        # --- Securely get the Google API Key from Kaggle Secrets ---\n    # This is the correct way for Kaggle/Colab notebooks\n    if \"GOOGLE_API_KEY\" not in os.environ:\n        try:\n            # For Kaggle Notebooks:\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            os.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n            print(\"*\"*50)\n            print(\"Google API Key loaded from Kaggle Secrets.\")\n            print(\"*\"*50)\n            \n            \n        except ImportError:\n            # Fallback for other environments if getpass also fails\n            logger.error(\"Google API Key not found. Please ensure it is set as a Kaggle Secret (or Colab Secret) named 'GOOGLE_API_KEY'.\")\n            # print(\"Kaggle Secrets not available. Trying to load from environment or it will error.\")\n    else:\n        loaded_key = os.environ.get(\"GOOGLE_API_KEY\")\n        logger.info(f\"DEBUG: GOOGLE_API_KEY is loaded. First 5 chars: {loaded_key[:5]}*****\")\n        logger.info(f\"DEBUG: Key length: {len(loaded_key)}\") # A Gemini key is usually 39 characters\n        \n\n    if not os.environ.get(\"GOOGLE_API_KEY\"):\n        logger.error(\"Google API Key not found. Please ensure it is set as a Kaggle Secret\")\n        raise ValueError(\"Google API Key not found. Please ensure it is set as a Kaggle Secret (or Colab Secret) named 'GOOGLE_API_KEY'.\")\n        \n    \n    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n\n    if not os.path.exists(faiss_db_path):\n        logger.info(f\"FAISS index not found at {faiss_db_path}. Creating it now...\")\n        loaded_docs = load_document(document)\n        text_chunks = split_documents(loaded_docs)\n        faiss_vector_store = create_vector_store(text_chunks, embedding_model, db_path=faiss_db_path)\n    else:\n        faiss_vector_store = load_vector_store(embedding_model, db_path=faiss_db_path)\n\n        loaded_docs = load_document(document)\n        text_chunks = split_documents(loaded_docs)\n    \n    fiass_retriever = faiss_vector_store.as_retriever(search_kwargs={\"k\": 5})\n    logger.info(\"FAISS retriever initialized (semantic search).\")\n    \n    # 3. Setup BM25 Retriever (Keyword Search Retriever)\n    # BM25Retriever is created directly from the raw text chunk\n\n    bm25_retriever = BM25Retriever.from_documents(text_chunks)\n    bm25_retriever.k = 5\n    logger.info(\"BM25 retriever initialized (keyword search).\")\n\n        \n    ensemble_retriever = EnsembleRetriever(\n        retrievers=[fiass_retriever,bm25_retriever],\n        weighs=[0.5,0.5]\n    )\n    logger.info(f\"Ensemble Retriever (Hybrid Search) initialized with weights {ensemble_retriever.weights}.\")\n    logger.info(f\"Ensemble Retriever will pass initial {bm25_retriever.k} documents to the re-ranker.\")\n\n    # 4. Setup Re-ranker\n    # re-ranker cross-encoder/ms-marco-MiniLM-L-6-v2' is a good general choice\n    # other options: 'cross-encoder/ms-marco-MMR' (for diversity), 'cross-encoder/ms-marco-TinyBERT-L-2' (smaller)\n\n    reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n    logger.info(f\"Loading re-ranker model: {reranker_model_name}...\")\n    try:\n        cross_encoder_model = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n        reranker = CrossEncoderReranker(model=cross_encoder_model, top_n=2)\n    except Exception as e:\n        logger.error(f\"Error loading re-ranker model '{reranker_model_name}': {e}\")\n        logger.error(\"Please ensure 'sentence-transformers' is installed and the model name is correct.\")\n        raise\n\n    # 5. Apply Contextual Compression with the Re-ranker\n    compression_retriever = ContextualCompressionRetriever(\n        base_compressor=reranker,\n        base_retriever=ensemble_retriever\n    )\n    logger.info(\"ContextualCompressionRetriever (with re-ranker) initialized.\")\n    \n    logger.info(f\"Initializing LLM: {llm_model_id} (Gemini)...\")\n    \n    llm = ChatGoogleGenerativeAI(\n            model=llm_model_id,#temperature=0.7,\n             \n        )\n    logger.info(f\"Successfully initialized Gemini model: {llm_model_id}\")\n    # --- Prompt Template ---\n    # This template works well for both Flan-T5 and Gemini\n    template = \"\"\"Context: {context}\n\n    Question: {question}\n\n    Based on the provided context, please provide a detailed and comprehensive answer. If the answer is not present in the context, state that you don't know.\n    Answer:\"\"\"\n    \n    prompt_template_obj = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n\n     # --- Create RetrievalQA Chain ---\n    # Use RetrievalQA.from_chain_type for simplicity in this structure\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\", # 'stuff' concatenates all retrieved docs into a single prompt\n        retriever=compression_retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt_template_obj}\n    )\n\n    return qa_chain,llm\n\nif __name__ == \"__main__\":\n   \n\n    \n    GEMINI_LLM_MODEL = \"models/gemini-2.0-flash\"\n\n    qa_chain,llm_ragas = setup_rag_components(document=\"/kaggle/input/ai-wiki/Artificial intelligence - Wikipedia.pdf\",\n                                   faiss_db_path=\"/kaggle/working/faiss_index_3\",\n                                   llm_model_id=GEMINI_LLM_MODEL)\n\n\n       ################# RAG Evaluation Setup####################\n\n    logger.info(\"Initiating Ragas evaluation...\")\n\n    answers = []\n    contexts = []\n    questions_list = [] # To store questions from the DataFrame\n    ground_truths_list = [] # To store ground_truths from the DataFrame\n\n    # reading the evaluation dataset\n    df_eval = pd.read_csv('/kaggle/input/ai-wiki-eval-set-csv/ai_wiki_eval_set.csv')\n    df_eval = df_eval.reset_index(drop=True)\n\n    for i, row in df_eval.iloc[:3].iterrows():\n        question = row['Question']\n        ground_truth = row['Answer'] # Get ground truth directly from DataFrame\n\n        questions_list.append(question)\n        ground_truths_list.append(ground_truth)\n\n        try:\n            response = qa_chain.invoke({\"query\": question})\n            answers.append(response['result'])\n            # Ensure contexts are a list of strings for Ragas\n            contexts.append([doc.page_content for doc in response[\"source_documents\"]])\n\n        except Exception as e:\n            logger.error(f\"Error during Ragas evaluation for question '{question}': {e}\")\n            # Append None or empty values to maintain list length\n            answers.append(None)\n            contexts.append([]) # Must be a list for Ragas, even if empty\n        time.sleep(0.5) # Pause for 500 milliseconds after each iteration\n    \n    temp_df = pd.DataFrame({\n        \"question\": questions_list,\n        \"ground_truth\": ground_truths_list,\n        \"answer\": answers,\n        \"contexts\": contexts,\n    })\n    \n    initial_rows = len(temp_df)\n    temp_df.dropna(subset=['question', 'ground_truth', 'answer', 'contexts'], inplace=True)\n    final_rows = len(temp_df)\n    if final_rows < initial_rows:\n        logger.warning(f\"Dropped {initial_rows - final_rows} rows from evaluation dataset due to missing values.\")\n    \n    # Convert the cleaned Pandas DataFrame to a Ragas Dataset\n    ragas_dataset = Dataset.from_pandas(temp_df)\n    \n    print(\"--- Cleaned Ragas Dataset for Evaluation ---\")\n    print(ragas_dataset)\n    gemini_eval_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0) # Set temp to 0 for determinism\n    ragas_llm = LangchainLLMWrapper(gemini_eval_llm)\n    logger.info(f\"Ragas evaluation LLM ({gemini_eval_llm.model}) initialized.\")\n\n    # Initialize Embeddings for Ragas evaluation (using Google Embeddings)\n    # Use a text embedding model suitable for Ragas\n    ragas_embeddings_google = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # Use the text embedding model\n    ragas_embeddings = LangchainEmbeddingsWrapper(ragas_embeddings_google)\n    logger.info(f\"Ragas evaluation Embeddings ({ragas_embeddings_google.model}) initialized.\")\n\n    \n    from ragas.run_config import RunConfig # Make sure this is imported at the top\n\n\n    ragas_run_config = RunConfig(\n        max_retries=5,\n        timeout=120,  # Maximum time for a single operation (e.g., LLM call)\n        max_wait=180,  # Max wait time between retries (if retries occur)\n        max_workers=2 # Start with a low number, adjust based on API limits and needs\n        # Removed 'thread_timeout' as it's no longer a valid argument\n    )\n\n    \n    # Run Ragas evaluation\n    logger.info(\"Running Ragas evaluate function...\")\n    results = evaluate(\n        dataset=ragas_dataset,\n        metrics=[faithfulness, answer_relevancy],\n        llm=gemini_eval_llm,\n        embeddings=ragas_embeddings,\n        raise_exceptions=True,\n        run_config=ragas_run_config,\n    )\n    print(\"\\n--- Ragas Evaluation Results ---\")\n    print(results)\n    print(\"\\n--- Ragas Evaluation Scores ---\")\n    # You can also get a pandas DataFrame of the results\n    results_df = results.to_pandas()\n    print(results_df)\n   \n    while True:\n        query = input(\"\\nEnter your question (type 'exit' to quit): \")\n        if query.lower() == 'exit':\n            break\n        \n        try:\n            # The qa_chain.invoke method uses the prompt template and LLM internally\n            response = qa_chain.invoke({\"query\": query})\n            \n            print(\"\\n--- Answer ---\")\n            print(response[\"result\"])\n            \n            print(\"\\n--- Source Documents ---\")\n            for i, doc in enumerate(response[\"source_documents\"]):\n                # Ensure metadata exists and page key is present\n                page_info = doc.metadata.get('page', 'N/A')\n                # st.markdown(f\"**Document {i+1} (Page: {page_info}):**\")\n                print(f\"Doc {i+1} (Page: {page_info}): {doc.page_content[:400]}...\") # Print first 400 chars\n                print(\"-\" * 30)\n\n        except Exception as e:\n            logger.error(f\"An error occurred: {e}\")\n            logger.info(\"Please ensure your Google API Key is correctly set as an environment variable.\")\n            import traceback\n            traceback.print_exc()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}